2022-11-19 16:38:58,241 [INFO] SPOTER(
  (transformer): Transformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): SPOTERTransformerDecoderLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): SPOTERTransformerDecoderLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): SPOTERTransformerDecoderLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (3): SPOTERTransformerDecoderLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (4): SPOTERTransformerDecoderLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (5): SPOTERTransformerDecoderLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)
          )
          (linear1): Linear(in_features=108, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=108, bias=True)
          (norm1): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((108,), eps=1e-05, elementwise_affine=True)
    )
  )
  (linear_class): Linear(in_features=108, out_features=100, bias=True)
)
2022-11-19 16:38:58,243 [INFO] Total number of trainable parameters: 5922772
2022-11-19 16:39:15,263 [INFO] Starting printcheck1...


2022-11-19 16:39:24,749 [INFO] Problematic normalization
2022-11-19 16:39:24,987 [INFO] Problematic normalization
2022-11-19 16:39:32,673 [INFO] Problematic normalization
2022-11-19 16:39:32,674 [INFO] Problematic normalization
2022-11-19 16:39:32,675 [INFO] Problematic normalization
2022-11-19 16:39:32,676 [INFO] Problematic normalization
2022-11-19 16:39:32,676 [INFO] Problematic normalization
2022-11-19 16:39:32,677 [INFO] Problematic normalization
2022-11-19 16:39:32,678 [INFO] Problematic normalization
2022-11-19 16:39:32,678 [INFO] Problematic normalization
2022-11-19 16:39:32,679 [INFO] Problematic normalization
2022-11-19 16:39:32,679 [INFO] Problematic normalization
2022-11-19 16:39:32,680 [INFO] Problematic normalization
2022-11-19 16:39:32,680 [INFO] Problematic normalization
2022-11-19 16:39:32,681 [INFO] Problematic normalization
2022-11-19 16:39:32,682 [INFO] Problematic normalization
2022-11-19 16:39:32,682 [INFO] Problematic normalization
2022-11-19 16:39:32,683 [INFO] Problematic normalization
2022-11-19 16:39:32,683 [INFO] Problematic normalization
2022-11-19 16:39:32,684 [INFO] Problematic normalization
2022-11-19 16:39:32,684 [INFO] Problematic normalization
2022-11-19 16:39:32,685 [INFO] Problematic normalization
2022-11-19 16:39:32,685 [INFO] Problematic normalization
2022-11-19 16:39:32,686 [INFO] Problematic normalization
2022-11-19 16:39:32,686 [INFO] Problematic normalization
2022-11-19 16:39:32,687 [INFO] Problematic normalization
2022-11-19 16:39:32,688 [INFO] Problematic normalization
2022-11-19 16:39:32,688 [INFO] Problematic normalization
2022-11-19 16:39:32,688 [INFO] Problematic normalization
2022-11-19 16:39:34,508 [INFO] Problematic normalization
2022-11-19 16:39:34,509 [INFO] Problematic normalization
2022-11-19 16:39:34,509 [INFO] Problematic normalization
2022-11-19 16:39:34,510 [INFO] Problematic normalization
2022-11-19 16:39:34,510 [INFO] Problematic normalization
2022-11-19 16:39:34,511 [INFO] Problematic normalization
2022-11-19 16:39:34,511 [INFO] Problematic normalization
2022-11-19 16:39:34,512 [INFO] Problematic normalization
2022-11-19 16:39:34,512 [INFO] Problematic normalization
2022-11-19 16:39:34,513 [INFO] Problematic normalization
2022-11-19 16:39:34,513 [INFO] Problematic normalization
2022-11-19 16:39:34,514 [INFO] Problematic normalization
2022-11-19 16:39:34,514 [INFO] Problematic normalization
2022-11-19 16:39:34,515 [INFO] Problematic normalization
2022-11-19 16:39:34,515 [INFO] Problematic normalization
2022-11-19 16:39:34,516 [INFO] Problematic normalization
2022-11-19 16:39:34,516 [INFO] Problematic normalization
2022-11-19 16:39:34,517 [INFO] Problematic normalization
2022-11-19 16:39:34,517 [INFO] Problematic normalization
2022-11-19 16:39:34,518 [INFO] Problematic normalization
2022-11-19 16:39:34,518 [INFO] Problematic normalization
2022-11-19 16:39:34,518 [INFO] Problematic normalization
2022-11-19 16:39:34,519 [INFO] Problematic normalization
2022-11-19 16:39:34,519 [INFO] Problematic normalization
2022-11-19 16:39:34,520 [INFO] Problematic normalization
2022-11-19 16:39:34,520 [INFO] Problematic normalization
2022-11-19 16:39:34,521 [INFO] Problematic normalization
2022-11-19 16:39:34,521 [INFO] Problematic normalization
2022-11-19 16:39:34,522 [INFO] Problematic normalization
2022-11-19 16:39:34,522 [INFO] Problematic normalization
2022-11-19 16:39:34,523 [INFO] Problematic normalization
2022-11-19 16:39:34,523 [INFO] Problematic normalization
2022-11-19 16:39:43,934 [INFO] Problematic normalization
2022-11-19 16:39:52,605 [INFO] Problematic normalization
2022-11-19 16:40:11,463 [INFO] Problematic normalization
2022-11-19 16:40:11,464 [INFO] Problematic normalization
2022-11-19 16:40:12,729 [INFO] Problematic normalization
2022-11-19 16:40:12,730 [INFO] Problematic normalization
2022-11-19 16:40:12,731 [INFO] Problematic normalization
2022-11-19 16:40:12,732 [INFO] Problematic normalization
2022-11-19 16:40:12,732 [INFO] Problematic normalization
2022-11-19 16:40:12,733 [INFO] Problematic normalization
2022-11-19 16:40:12,733 [INFO] Problematic normalization
2022-11-19 16:40:12,734 [INFO] Problematic normalization
2022-11-19 16:40:12,734 [INFO] Problematic normalization
2022-11-19 16:40:12,734 [INFO] Problematic normalization
2022-11-19 16:40:12,735 [INFO] Problematic normalization
2022-11-19 16:40:12,735 [INFO] Problematic normalization
2022-11-19 16:40:12,736 [INFO] Problematic normalization
2022-11-19 16:40:12,736 [INFO] Problematic normalization
2022-11-19 16:40:12,737 [INFO] Problematic normalization
2022-11-19 16:40:12,737 [INFO] Problematic normalization
2022-11-19 16:40:12,738 [INFO] Problematic normalization
2022-11-19 16:40:12,738 [INFO] Problematic normalization
2022-11-19 16:40:12,739 [INFO] Problematic normalization
2022-11-19 16:40:12,739 [INFO] Problematic normalization
2022-11-19 16:40:12,740 [INFO] Problematic normalization
2022-11-19 16:40:12,740 [INFO] Problematic normalization
2022-11-19 16:40:12,741 [INFO] Problematic normalization
2022-11-19 16:40:12,741 [INFO] Problematic normalization
2022-11-19 16:40:12,742 [INFO] Problematic normalization
2022-11-19 16:40:12,742 [INFO] Problematic normalization
2022-11-19 16:40:12,743 [INFO] Problematic normalization
2022-11-19 16:40:12,743 [INFO] Problematic normalization
2022-11-19 16:40:12,744 [INFO] Problematic normalization
2022-11-19 16:40:12,744 [INFO] Problematic normalization
2022-11-19 16:40:12,744 [INFO] Problematic normalization
2022-11-19 16:40:12,745 [INFO] Problematic normalization
2022-11-19 16:40:12,746 [INFO] Problematic normalization
2022-11-19 16:40:12,746 [INFO] Problematic normalization
2022-11-19 16:40:12,746 [INFO] Problematic normalization
2022-11-19 16:40:12,747 [INFO] Problematic normalization
2022-11-19 16:40:12,748 [INFO] Problematic normalization
2022-11-19 16:40:12,748 [INFO] Problematic normalization
2022-11-19 16:40:12,749 [INFO] Problematic normalization
2022-11-19 16:40:12,749 [INFO] Problematic normalization
2022-11-19 16:40:12,750 [INFO] Problematic normalization
2022-11-19 16:40:12,750 [INFO] Problematic normalization
2022-11-19 16:40:12,751 [INFO] Problematic normalization
2022-11-19 16:40:12,751 [INFO] Problematic normalization
2022-11-19 16:40:12,752 [INFO] Problematic normalization
2022-11-19 16:40:12,752 [INFO] Problematic normalization
2022-11-19 16:40:12,753 [INFO] Problematic normalization
2022-11-19 16:40:12,753 [INFO] Problematic normalization
2022-11-19 16:40:12,754 [INFO] Problematic normalization
2022-11-19 16:40:12,754 [INFO] Problematic normalization
2022-11-19 16:40:12,754 [INFO] Problematic normalization
2022-11-19 16:40:12,755 [INFO] Problematic normalization
2022-11-19 16:40:12,755 [INFO] Problematic normalization
2022-11-19 16:40:12,756 [INFO] Problematic normalization
2022-11-19 16:40:12,756 [INFO] Problematic normalization
2022-11-19 16:40:12,757 [INFO] Problematic normalization
2022-11-19 16:40:12,757 [INFO] Problematic normalization
2022-11-19 16:40:12,757 [INFO] Problematic normalization
2022-11-19 16:40:12,758 [INFO] Problematic normalization
2022-11-19 16:40:17,351 [INFO] Problematic normalization
2022-11-19 16:40:17,353 [INFO] Problematic normalization
2022-11-19 16:40:22,047 [INFO] Problematic normalization
2022-11-19 16:40:27,048 [INFO] Problematic normalization
2022-11-19 16:40:27,734 [INFO] Problematic normalization
2022-11-19 16:40:28,956 [INFO] Problematic normalization
2022-11-19 16:40:28,957 [INFO] Problematic normalization
2022-11-19 16:40:28,958 [INFO] Problematic normalization
2022-11-19 16:40:28,958 [INFO] Problematic normalization
2022-11-19 16:40:28,959 [INFO] Problematic normalization
2022-11-19 16:40:28,959 [INFO] Problematic normalization
2022-11-19 16:40:28,960 [INFO] Problematic normalization
2022-11-19 16:40:28,961 [INFO] Problematic normalization
2022-11-19 16:40:28,961 [INFO] Problematic normalization
2022-11-19 16:40:28,962 [INFO] Problematic normalization
2022-11-19 16:40:28,963 [INFO] Problematic normalization
2022-11-19 16:40:28,964 [INFO] Problematic normalization
2022-11-19 16:40:28,964 [INFO] Problematic normalization
2022-11-19 16:40:28,965 [INFO] Problematic normalization
2022-11-19 16:40:28,966 [INFO] Problematic normalization
2022-11-19 16:40:28,967 [INFO] Problematic normalization
2022-11-19 16:40:28,968 [INFO] Problematic normalization
2022-11-19 16:40:28,968 [INFO] Problematic normalization
2022-11-19 16:40:28,969 [INFO] Problematic normalization
2022-11-19 16:40:28,970 [INFO] Problematic normalization
2022-11-19 16:40:28,971 [INFO] Problematic normalization
2022-11-19 16:40:28,972 [INFO] Problematic normalization
2022-11-19 16:40:28,972 [INFO] Problematic normalization
2022-11-19 16:40:28,973 [INFO] Problematic normalization
2022-11-19 16:40:36,058 [INFO] Problematic normalization
2022-11-19 16:40:52,752 [INFO] [1] TRAIN  loss: 4.805483242675104 acc: 0.008321775312066574
2022-11-19 16:40:52,753 [INFO] [1] VALIDATION  acc: 0.02967359050445104
2022-11-19 16:40:52,754 [INFO] 
2022-11-19 16:40:52,754 [INFO] 
Testing checkpointed models starting...

2022-11-19 16:40:58,629 [INFO] Label accuracies statistics:
2022-11-19 16:40:58,629 [INFO] {0: 1.0, 1: 0.5, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0, 41: 0.0, 42: 0.0, 43: 0.0, 44: 0.0, 45: 0.0, 46: 0.0, 47: 0.0, 48: 0.0, 49: 0.0, 50: 0.0, 51: 0.0, 52: 0.0, 53: 0.0, 54: 0.0, 55: 0.0, 56: 0.0, 57: 0.0, 58: 0.0, 59: 0.0, 60: 0.0, 61: 0.0, 62: 0.0, 63: 0.0, 64: 0.0, 65: 0.0, 66: 0.0, 67: 0.0, 68: 0.0, 69: 0.0, 70: 0.0, 71: 0.0, 72: 0.0, 73: 0.0, 74: 0.0, 75: 0.0, 76: 0.0, 77: 0.0, 78: 0.0, 79: 0.0, 80: 0.0, 81: 0.0, 82: 0.0, 83: 0.0, 84: 0.0, 85: 0.0, 86: 0.0, 87: 0.0, 88: 0.0, 89: 0.0, 90: 0.0, 91: 0.0, 92: 0.0, 93: 0.0, 94: 0.0, 95: 0.0, 96: 0.0, 97: 0.0, 98: 0.0, 99: 0.0}

2022-11-19 16:40:58,631 [INFO] checkpoint_t_0  ->  0.023255813953488372
2022-11-19 16:41:04,704 [INFO] Label accuracies statistics:
2022-11-19 16:41:04,704 [INFO] {0: 1.0, 1: 0.5, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.0, 19: 0.0, 20: 0.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 25: 0.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 0.0, 30: 0.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 0.0, 40: 0.0, 41: 0.0, 42: 0.0, 43: 0.0, 44: 0.0, 45: 0.0, 46: 0.0, 47: 0.0, 48: 0.0, 49: 0.0, 50: 0.0, 51: 0.0, 52: 0.0, 53: 0.0, 54: 0.0, 55: 0.0, 56: 0.0, 57: 0.0, 58: 0.0, 59: 0.0, 60: 0.0, 61: 0.0, 62: 0.0, 63: 0.0, 64: 0.0, 65: 0.0, 66: 0.0, 67: 0.0, 68: 0.0, 69: 0.0, 70: 0.0, 71: 0.0, 72: 0.0, 73: 0.0, 74: 0.0, 75: 0.0, 76: 0.0, 77: 0.0, 78: 0.0, 79: 0.0, 80: 0.0, 81: 0.0, 82: 0.0, 83: 0.0, 84: 0.0, 85: 0.0, 86: 0.0, 87: 0.0, 88: 0.0, 89: 0.0, 90: 0.0, 91: 0.0, 92: 0.0, 93: 0.0, 94: 0.0, 95: 0.0, 96: 0.0, 97: 0.0, 98: 0.0, 99: 0.0}

2022-11-19 16:41:04,706 [INFO] checkpoint_v_0  ->  0.023255813953488372
2022-11-19 16:41:04,706 [INFO] 
The top result was recorded at 0.023255813953488372 testing accuracy. The best checkpoint is printcheck1/checkpoint_t_0.
2022-11-19 16:41:04,965 [INFO] 
Any desired statistics have been plotted.
The experiment is finished.
